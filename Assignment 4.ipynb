{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train: (50000, 32, 32, 3)\n",
      "x_test: (10000, 32, 32, 3)\n",
      "w1: (3072, 10)\n",
      "b1: (10,)\n",
      "Epoch 1/100>>> trainig_loss:  0.5000011469878922 testing_loss: 0.4411523414506603\n",
      "Epoch 2/100>>> trainig_loss:  0.4421280447133633 testing_loss: 0.4290877051873943\n",
      "Epoch 3/100>>> trainig_loss:  0.4293999247897458 testing_loss: 0.41166370237622524\n",
      "Epoch 4/100>>> trainig_loss:  0.4120741842243922 testing_loss: 0.4016330864682733\n",
      "Epoch 5/100>>> trainig_loss:  0.4019206746691254 testing_loss: 0.39192934300516447\n",
      "Epoch 6/100>>> trainig_loss:  0.39220842705771747 testing_loss: 0.38409894245098813\n",
      "Epoch 7/100>>> trainig_loss:  0.3843406723491497 testing_loss: 0.37696859938137334\n",
      "Epoch 8/100>>> trainig_loss:  0.3771915260019078 testing_loss: 0.37062634515927473\n",
      "Epoch 9/100>>> trainig_loss:  0.3708273918386447 testing_loss: 0.3648113337241597\n",
      "Epoch 10/100>>> trainig_loss:  0.3649938711820334 testing_loss: 0.3594665793057578\n",
      "Epoch 11/100>>> trainig_loss:  0.3596305826273817 testing_loss: 0.35450196612434576\n",
      "Epoch 12/100>>> trainig_loss:  0.35464822380752786 testing_loss: 0.3498682421469235\n",
      "Epoch 13/100>>> trainig_loss:  0.34999699061629513 testing_loss: 0.34551984548688064\n",
      "Epoch 14/100>>> trainig_loss:  0.3456314190579771 testing_loss: 0.34142332981209916\n",
      "Epoch 15/100>>> trainig_loss:  0.34151797996225136 testing_loss: 0.3375505023726651\n",
      "Epoch 16/100>>> trainig_loss:  0.337628500053724 testing_loss: 0.33387855138817363\n",
      "Epoch 17/100>>> trainig_loss:  0.3339401622912923 testing_loss: 0.330388266867676\n",
      "Epoch 18/100>>> trainig_loss:  0.33043376488605675 testing_loss: 0.3270634715072126\n",
      "Epoch 19/100>>> trainig_loss:  0.3270931322840441 testing_loss: 0.3238903203227925\n",
      "Epoch 20/100>>> trainig_loss:  0.32390441997937985 testing_loss: 0.32085687862355305\n",
      "Epoch 21/100>>> trainig_loss:  0.3208556897250009 testing_loss: 0.3179527570653068\n",
      "Epoch 22/100>>> trainig_loss:  0.31793654575737035 testing_loss: 0.3151688435560401\n",
      "Epoch 23/100>>> trainig_loss:  0.31513786681979034 testing_loss: 0.31249708612897126\n",
      "Epoch 24/100>>> trainig_loss:  0.31245158974150106 testing_loss: 0.3099303224510268\n",
      "Epoch 25/100>>> trainig_loss:  0.30987053946092086 testing_loss: 0.3074621418997823\n",
      "Epoch 26/100>>> trainig_loss:  0.3073882916417775 testing_loss: 0.3050867740498547\n",
      "Epoch 27/100>>> trainig_loss:  0.3049990616006248 testing_loss: 0.30279899736075727\n",
      "Epoch 28/100>>> trainig_loss:  0.30269761336988854 testing_loss: 0.3005940639968164\n",
      "Epoch 29/100>>> trainig_loss:  0.3004791848145451 testing_loss: 0.29846763743409077\n",
      "Epoch 30/100>>> trainig_loss:  0.2983394254697612 testing_loss: 0.2964157403646297\n",
      "Epoch 31/100>>> trainig_loss:  0.2962743446163675 testing_loss: 0.29443471092397183\n",
      "Epoch 32/100>>> trainig_loss:  0.2942802676300499 testing_loss: 0.29252116570410547\n",
      "Epoch 33/100>>> trainig_loss:  0.2923537990749311 testing_loss: 0.29067196832856856\n",
      "Epoch 34/100>>> trainig_loss:  0.29049179132629355 testing_loss: 0.28888420261529485\n",
      "Epoch 35/100>>> trainig_loss:  0.28869131775495477 testing_loss: 0.2871551495433927\n",
      "Epoch 36/100>>> trainig_loss:  0.2869496496954257 testing_loss: 0.2854822673902519\n",
      "Epoch 37/100>>> trainig_loss:  0.2852642365691837 testing_loss: 0.28386317452345733\n",
      "Epoch 38/100>>> trainig_loss:  0.28363268865159885 testing_loss: 0.28229563442594907\n",
      "Epoch 39/100>>> trainig_loss:  0.28205276206423496 testing_loss: 0.28077754260794036\n",
      "Epoch 40/100>>> trainig_loss:  0.2805223456486544 testing_loss: 0.27930691511951067\n",
      "Epoch 41/100>>> trainig_loss:  0.27903944943770076 testing_loss: 0.27788187842664674\n",
      "Epoch 42/100>>> trainig_loss:  0.2776021944886274 testing_loss: 0.27650066045321486\n",
      "Epoch 43/100>>> trainig_loss:  0.2762088038817712 testing_loss: 0.2751615826237836\n",
      "Epoch 44/100>>> trainig_loss:  0.27485759472059745 testing_loss: 0.273863052768816\n",
      "Epoch 45/100>>> trainig_loss:  0.27354697099529923 testing_loss: 0.2726035587756562\n",
      "Epoch 46/100>>> trainig_loss:  0.27227541719384024 testing_loss: 0.2713816628868381\n",
      "Epoch 47/100>>> trainig_loss:  0.27104149256228316 testing_loss: 0.2701959965622612\n",
      "Epoch 48/100>>> trainig_loss:  0.2698438259311404 testing_loss: 0.2690452558342719\n",
      "Epoch 49/100>>> trainig_loss:  0.26868111103688747 testing_loss: 0.26792819709511945\n",
      "Epoch 50/100>>> trainig_loss:  0.2675521022781396 testing_loss: 0.2668436332649883\n",
      "Epoch 51/100>>> trainig_loss:  0.26645561085467506 testing_loss: 0.2657904302961445\n",
      "Epoch 52/100>>> trainig_loss:  0.2653905012447826 testing_loss: 0.2647675039749054\n",
      "Epoch 53/100>>> trainig_loss:  0.26435568798256115 testing_loss: 0.26377381698836383\n",
      "Epoch 54/100>>> trainig_loss:  0.26335013270200114 testing_loss: 0.2628083762272109\n",
      "Epoch 55/100>>> trainig_loss:  0.26237284141907596 testing_loss: 0.2618702302997534\n",
      "Epoch 56/100>>> trainig_loss:  0.26142286202682713 testing_loss: 0.26095846723541916\n",
      "Epoch 57/100>>> trainig_loss:  0.2604992819816107 testing_loss: 0.26007221235876776\n",
      "Epoch 58/100>>> trainig_loss:  0.2596012261614038 testing_loss: 0.2592106263173608\n",
      "Epoch 59/100>>> trainig_loss:  0.25872785487941125 testing_loss: 0.2583729032488502\n",
      "Epoch 60/100>>> trainig_loss:  0.257878362038216 testing_loss: 0.2575582690743688\n",
      "Epoch 61/100>>> trainig_loss:  0.2570519734114511 testing_loss: 0.25676597990679706\n",
      "Epoch 62/100>>> trainig_loss:  0.25624794504146553 testing_loss: 0.25599532056376856\n",
      "Epoch 63/100>>> trainig_loss:  0.25546556174275287 testing_loss: 0.2552456031763953\n",
      "Epoch 64/100>>> trainig_loss:  0.25470413570203354 testing_loss: 0.2545161658856675\n",
      "Epoch 65/100>>> trainig_loss:  0.2539630051668655 testing_loss: 0.2538063716193326\n",
      "Epoch 66/100>>> trainig_loss:  0.2532415332155075 testing_loss: 0.25311560694279833\n",
      "Epoch 67/100>>> trainig_loss:  0.25253910660151657 testing_loss: 0.2524432809782596\n",
      "Epoch 68/100>>> trainig_loss:  0.25185513466720727 testing_loss: 0.25178882438681893\n",
      "Epoch 69/100>>> trainig_loss:  0.2511890483206904 testing_loss: 0.2511516884088773\n",
      "Epoch 70/100>>> trainig_loss:  0.2505402990717085 testing_loss: 0.25053134395851734\n",
      "Epoch 71/100>>> trainig_loss:  0.24990835812194392 testing_loss: 0.24992728076799636\n",
      "Epoch 72/100>>> trainig_loss:  0.24929271550587037 testing_loss: 0.24933900657881955\n",
      "Epoch 73/100>>> trainig_loss:  0.2486928792785778 testing_loss: 0.24876604637617417\n",
      "Epoch 74/100>>> trainig_loss:  0.24810837474731415 testing_loss: 0.24820794166378685\n",
      "Epoch 75/100>>> trainig_loss:  0.24753874374377108 testing_loss: 0.24766424977651447\n",
      "Epoch 76/100>>> trainig_loss:  0.24698354393439487 testing_loss: 0.247134543228204\n",
      "Epoch 77/100>>> trainig_loss:  0.24644234816622887 testing_loss: 0.24661840909255675\n",
      "Epoch 78/100>>> trainig_loss:  0.2459147438459964 testing_loss: 0.24611544841491598\n",
      "Epoch 79/100>>> trainig_loss:  0.24540033235032277 testing_loss: 0.24562527565305647\n",
      "Epoch 80/100>>> trainig_loss:  0.24489872846515115 testing_loss: 0.24514751814520694\n",
      "Epoch 81/100>>> trainig_loss:  0.24440955985256668 testing_loss: 0.2446818156036679\n",
      "Epoch 82/100>>> trainig_loss:  0.24393246654337206 testing_loss: 0.2442278196325077\n",
      "Epoch 83/100>>> trainig_loss:  0.24346710045388242 testing_loss: 0.24378519326793394\n",
      "Epoch 84/100>>> trainig_loss:  0.2430131249255252 testing_loss: 0.24335361054003601\n",
      "Epoch 85/100>>> trainig_loss:  0.2425702142859214 testing_loss: 0.24293275605468412\n",
      "Epoch 86/100>>> trainig_loss:  0.24213805343023095 testing_loss: 0.24252232459446055\n",
      "Epoch 87/100>>> trainig_loss:  0.24171633742161924 testing_loss: 0.24212202073756964\n",
      "Epoch 88/100>>> trainig_loss:  0.24130477110978663 testing_loss: 0.2417315584937473\n",
      "Epoch 89/100>>> trainig_loss:  0.24090306876657192 testing_loss: 0.24135066095625426\n",
      "Epoch 90/100>>> trainig_loss:  0.24051095373770517 testing_loss: 0.24097905996909785\n",
      "Epoch 91/100>>> trainig_loss:  0.24012815810984955 testing_loss: 0.24061649580868122\n",
      "Epoch 92/100>>> trainig_loss:  0.23975442239212236 testing_loss: 0.24026271687913073\n",
      "Epoch 93/100>>> trainig_loss:  0.23938949521134278 testing_loss: 0.23991747942059774\n",
      "Epoch 94/100>>> trainig_loss:  0.2390331330202949 testing_loss: 0.239580547229876\n",
      "Epoch 95/100>>> trainig_loss:  0.23868509981834424 testing_loss: 0.23925169139271485\n",
      "Epoch 96/100>>> trainig_loss:  0.23834516688378293 testing_loss: 0.23893069002724548\n",
      "Epoch 97/100>>> trainig_loss:  0.23801311251731672 testing_loss: 0.2386173280379736\n",
      "Epoch 98/100>>> trainig_loss:  0.23768872179614428 testing_loss: 0.2383113968798222\n",
      "Epoch 99/100>>> trainig_loss:  0.23737178633810885 testing_loss: 0.23801269433173922\n",
      "Epoch 100/100>>> trainig_loss:  0.23706210407543385 testing_loss: 0.23772102427941136\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train:' , x_train.shape)\n",
    "print('x_test:' , x_test.shape)\n",
    "K = len(np.unique(y_train)) # Classes\n",
    "Ntr = x_train.shape[0]\n",
    "\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "#https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current\n",
    "x_test = x_test - mean_image\n",
    "\"\"\"In the ytrain it is preliminarily is in numbers just to indicate the particular class tf.keras.utils.to_categorical(y_train, num_classes=K) does is \n",
    "Converting the number to a binary vector\n",
    "Ex: \n",
    "a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "a = tf.constant(a, shape=[4, 4])\n",
    "print(a)\n",
    "tf.Tensor(\n",
    "[[1. 0. 0. 0.]\n",
    " [0. 1. 0. 0.]\n",
    " [0. 0. 1. 0.]\n",
    " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n",
    "\"\"\"\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "x_train = np.reshape(x_train,(Ntr,Din))#shaping images into a vector Din=32x32x3\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "std=1e-5\n",
    "w1 = std*np.random.randn(Din, K)#initialization\n",
    "b1 = np.zeros(K)\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "batch_size = Ntr #for the batch size whole dataset is considered\n",
    "\n",
    "iterations =100\n",
    "lr =0.025\n",
    "lr_decay=0.001#learning rate is decreased in each iteration\n",
    "reg =0#regularization term which stops growth of weights\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "for t in range(iterations):\n",
    "    #indices = np.arange(Ntr)#generating image\n",
    "    #rng.shuffle(indices)\n",
    "    # Forward pass\n",
    "    loss,cache=forward_prop(W=w1,b=b1,X=x_train,Y=y_train,reg=reg)\n",
    "    y_hat=cache[\"y_hat\"]\n",
    "    # Backward pass\n",
    "    dw1,db1=back_prop(Y_hat=y_hat,W=w1,X=x_train,Y=y_train,reg=reg)\n",
    "    w1=w1-lr*dw1\n",
    "    b1=b1-lr*db1\n",
    "\n",
    "    test_loss,_=forward_prop(W=w1,b=b1,X=x_test,Y=y_test,reg=reg)\n",
    "    print(\"Epoch \"+str(t+1)+\"/\"+str(iterations)+\">>> trainig_loss: \",loss,\"testing_loss:\",test_loss)\n",
    "# Printing accuracies and displaying w as images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n",
      "[1.5 0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "a = tf.constant(a, shape=[4, 4])\n",
    "print(a)\n",
    "B=[[1,2,1,1],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\n",
    "mse = np.mean((a - B)**2,axis=1)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "def loss(W,b,X,Y,reg):\n",
    "    print(W.shape,\"W\")\n",
    "    print(b.shape,\"b\")\n",
    "    print(X.shape,\"X\")\n",
    "    print(Y.shape,\"Y\")\n",
    "    prediction=np.dot(X,W)+np.transpose(b)\n",
    "    print()\n",
    "    m=X.shape[0]\n",
    "    mse = np.sum(np.square((Y - prediction)**2))\n",
    "    Loss=mse+ 1/(2*m)*reg*np.sum(W**2)\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.73105858, 0.88079708, 0.73105858, 0.73105858],\n",
       "       [0.5       , 0.73105858, 0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       , 0.73105858, 0.5       ],\n",
       "       [0.5       , 0.5       , 0.5       , 0.73105858]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    z=np.array(z)\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "sigmoid(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,Y,b,W,reg):\n",
    "    #print(W.shape,\"W\")\n",
    "    #print(b.shape,\"b\")\n",
    "    #print(X.shape,\"X\")\n",
    "    #print(Y.shape,\"Y\")\n",
    "    y_hat=np.dot(X,W)+np.transpose(b)\n",
    "    #print(A.shape,\"A\")\n",
    "    m=X.shape[0]\n",
    "    #print(m)\n",
    "    mse =1/(2*m)* np.sum(np.square((Y - y_hat)**2))\n",
    "    Loss=mse+ 1/(2*m)*reg*np.sum(W**2)\n",
    "    cache={\"y_hat\":y_hat}\n",
    "    return Loss,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.49998923857004474\n"
     ]
    }
   ],
   "source": [
    "loss,cache=forward_prop(W=w1,b=b1,X=x_train,Y=y_train,reg=reg)\n",
    "print(loss)\n",
    "y_hat=cache[\"y_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def back_prop(W,X,Y,Y_hat,reg):\n",
    "    m=Y.shape[0]\n",
    "    dy_hat =Y_hat-Y\n",
    "    #print(dy_hat.shape,\"dy_hat\")\n",
    "    #print(dA.shape,\"dA\")\n",
    "    dW =(1/float(m))*(np.dot(X.T, dy_hat)+reg*W)\n",
    "    db=(1/float(m))*np.sum(dy_hat,axis=0)\n",
    "    #print(dW.shape)\n",
    "    #print(db.shape)\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[-5.60294253e-03, -2.38228143e-03,  2.25626789e-03, ...,\n",
       "         -1.66584013e-03, -6.09623615e-03, -1.43466064e-02],\n",
       "        [-9.87432428e-03, -8.76206678e-04,  1.98903826e-03, ...,\n",
       "         -2.10664917e-03, -1.04874780e-02, -1.61687181e-02],\n",
       "        [-1.79099455e-02, -1.37219492e-03,  6.71838721e-03, ...,\n",
       "         -1.92025827e-03, -1.81712554e-02, -2.04092694e-02],\n",
       "        ...,\n",
       "        [-1.25619369e-03, -2.77595903e-03,  5.82931016e-04, ...,\n",
       "         -5.32556245e-03,  1.18562262e-02, -5.05095503e-03],\n",
       "        [-3.97208294e-03, -2.21981253e-03, -4.58652880e-05, ...,\n",
       "         -3.55784550e-03,  6.31214200e-03, -3.90185596e-03],\n",
       "        [-9.14413372e-03, -4.84109105e-03,  2.94591984e-03, ...,\n",
       "          2.34340162e-03, -1.38188891e-03, -5.50754172e-03]]),\n",
       " array([-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1]))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "back_prop(Y_hat=y_hat,W=w1,X=x_train,Y=y_train,reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
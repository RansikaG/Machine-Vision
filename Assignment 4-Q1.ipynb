{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,Y,b,W,reg):\n",
    "    y_hat=np.dot(X,W)+np.transpose(b)\n",
    "    m=X.shape[0]\n",
    "    mse =1/m* np.sum(np.square((Y - y_hat)**2))\n",
    "    Loss=mse+ reg*np.sum(W**2)\n",
    "    cache={\"y_hat\":y_hat}\n",
    "    return Loss,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def back_prop(W,X,Y,Y_hat,reg):\n",
    "    m=Y.shape[0]\n",
    "    dy_hat =Y_hat-Y\n",
    "    dW =(1/float(m))*(np.dot(X.T, dy_hat)+reg*W)\n",
    "    db=(1/float(m))*np.sum(dy_hat,axis=0)\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y):\n",
    "    y_hat = np.argmax(y_hat,axis=1)\n",
    "    y= np.argmax(y,axis=1)\n",
    "    accuracy=np.sum(y_hat==y)/y.size \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train: (50000, 32, 32, 3)\n",
      "x_test: (10000, 32, 32, 3)\n",
      "w1: (3072, 10)\n",
      "b1: (10,)\n",
      "Epoch 1/300>>> trainig_loss:  1.000077566582165 >>training_accuracy:  0.07956 testing_loss: 0.848296376315914 testing_accuracy: 0.2486\n",
      "Epoch 11/300>>> trainig_loss:  0.69618021174751 >>training_accuracy:  0.35628 testing_loss: 0.6608722273079519 testing_accuracy: 0.3528\n",
      "Epoch 21/300>>> trainig_loss:  0.606387857577094 >>training_accuracy:  0.37814 testing_loss: 0.5894243870587478 testing_accuracy: 0.3765\n",
      "Epoch 31/300>>> trainig_loss:  0.5546619514543816 >>training_accuracy:  0.38806 testing_loss: 0.5460990452604106 testing_accuracy: 0.3868\n",
      "Epoch 41/300>>> trainig_loss:  0.521867037171497 >>training_accuracy:  0.39374 testing_loss: 0.5176516872600654 testing_accuracy: 0.3907\n",
      "Epoch 51/300>>> trainig_loss:  0.500083639565314 >>training_accuracy:  0.39736 testing_loss: 0.4983142793085598 testing_accuracy: 0.3919\n",
      "Epoch 61/300>>> trainig_loss:  0.4851508575903444 >>training_accuracy:  0.4009 testing_loss: 0.4848715825352737 testing_accuracy: 0.3944\n",
      "Epoch 71/300>>> trainig_loss:  0.4746542018537746 >>training_accuracy:  0.40316 testing_loss: 0.4753585418372999 testing_accuracy: 0.3962\n",
      "Epoch 81/300>>> trainig_loss:  0.46711072105058715 >>training_accuracy:  0.40526 testing_loss: 0.4685172397134513 testing_accuracy: 0.3957\n",
      "Epoch 91/300>>> trainig_loss:  0.4615743966235021 >>training_accuracy:  0.40756 testing_loss: 0.46351928196856906 testing_accuracy: 0.396\n",
      "Epoch 101/300>>> trainig_loss:  0.4574251005390988 >>training_accuracy:  0.40968 testing_loss: 0.4598084590132203 testing_accuracy: 0.3979\n",
      "Epoch 111/300>>> trainig_loss:  0.4542479998963854 >>training_accuracy:  0.411 testing_loss: 0.4570059773381873 testing_accuracy: 0.3987\n",
      "Epoch 121/300>>> trainig_loss:  0.45176122099400173 >>training_accuracy:  0.41232 testing_loss: 0.45485102767150587 testing_accuracy: 0.3996\n",
      "Epoch 131/300>>> trainig_loss:  0.4497708258825758 >>training_accuracy:  0.41328 testing_loss: 0.45316242821543723 testing_accuracy: 0.4009\n",
      "Epoch 141/300>>> trainig_loss:  0.4481419768515537 >>training_accuracy:  0.41448 testing_loss: 0.45181334058353734 testing_accuracy: 0.4016\n",
      "Epoch 151/300>>> trainig_loss:  0.4467800561647843 >>training_accuracy:  0.41498 testing_loss: 0.45071433740499733 testing_accuracy: 0.4026\n",
      "Epoch 161/300>>> trainig_loss:  0.4456180913307974 >>training_accuracy:  0.41606 testing_loss: 0.4498019267944861 testing_accuracy: 0.4031\n",
      "Epoch 171/300>>> trainig_loss:  0.4446082686115347 >>training_accuracy:  0.41724 testing_loss: 0.44903070387855126 testing_accuracy: 0.4032\n",
      "Epoch 181/300>>> trainig_loss:  0.44371614617797017 >>training_accuracy:  0.41814 testing_loss: 0.44836794475463904 testing_accuracy: 0.4044\n",
      "Epoch 191/300>>> trainig_loss:  0.44291667559754616 >>training_accuracy:  0.41876 testing_loss: 0.44778986154709494 testing_accuracy: 0.4053\n",
      "Epoch 201/300>>> trainig_loss:  0.44219144815477934 >>training_accuracy:  0.41924 testing_loss: 0.4472789957887756 testing_accuracy: 0.4062\n",
      "Epoch 211/300>>> trainig_loss:  0.44152677804040064 >>training_accuracy:  0.41966 testing_loss: 0.44682239651051836 testing_accuracy: 0.4054\n",
      "Epoch 221/300>>> trainig_loss:  0.4409123613003454 >>training_accuracy:  0.42016 testing_loss: 0.4464103418367774 testing_accuracy: 0.4055\n",
      "Epoch 231/300>>> trainig_loss:  0.44034033313681653 >>training_accuracy:  0.42034 testing_loss: 0.44603543850578603 testing_accuracy: 0.4057\n",
      "Epoch 241/300>>> trainig_loss:  0.43980460212187833 >>training_accuracy:  0.4209 testing_loss: 0.4456919850741909 testing_accuracy: 0.4059\n",
      "Epoch 251/300>>> trainig_loss:  0.4393003777018435 >>training_accuracy:  0.42162 testing_loss: 0.44537551967211286 testing_accuracy: 0.4059\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('x_train:' , x_train.shape)\n",
    "print('x_test:' , x_test.shape)\n",
    "K = len(np.unique(y_train)) # Classes\n",
    "Ntr = x_train.shape[0]\n",
    "\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "x_train = np.reshape(x_train,(Ntr,Din))#shaping images into a vector Din=32x32x3\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "std=1e-5\n",
    "w1 = std*np.random.randn(Din, K)#initialization\n",
    "b1 = np.zeros(K)\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "batch_size = Ntr #for the batch size whole dataset is considered\n",
    "\n",
    "iterations =300\n",
    "lr =0.035\n",
    "lr_decay=0#learning rate is decreased in each iteration\n",
    "reg =0#regularization term which stops growth of weights\n",
    "train_loss_history = []\n",
    "test_loss_history=[]\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)#generating image\n",
    "    rng.shuffle(indices)\n",
    "    x_train=x_train[indices]\n",
    "    y_train=y_train[indices]\n",
    "    # Forward pass\n",
    "    loss,cache=forward_prop(W=w1,b=b1,X=x_train,Y=y_train,reg=reg)\n",
    "    train_loss_history.append(loss)\n",
    "    y_hat=cache[\"y_hat\"]\n",
    "\n",
    "    train_acc=accuracy(y_hat,y_train)\n",
    "    train_acc_history.append(train_acc)\n",
    "    # Backward pass\n",
    "    dw1,db1=back_prop(Y_hat=y_hat,W=w1,X=x_train,Y=y_train,reg=reg)\n",
    "    w1-=lr*dw1\n",
    "    b1-=lr*db1\n",
    "    lr*=lr_decay\n",
    "    \n",
    "    test_loss,cache_test=forward_prop(W=w1,b=b1,X=x_test,Y=y_test,reg=reg)\n",
    "    y_test_hat=cache_test[\"y_hat\"]\n",
    "\n",
    "    test_acc=accuracy(y_test_hat,y_test)\n",
    "    test_acc_history.append(test_acc)\n",
    "    if t%10==0:\n",
    "        print(\"Epoch \"+str(t+1)+\"/\"+str(iterations)+\">>> trainig_loss: \",loss,\">>training_accuracy: \",train_acc,\"testing_loss:\",test_loss,\"testing_accuracy:\",test_acc)\n",
    "# Printing accuracies and displaying w as images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cc14bcae16ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#display Train loss,train accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#display Train loss,train accuracy\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "ax[0].plot(train_loss_history,'b')\n",
    "ax[0].plot(test_loss_history,'r')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')  \n",
    "ax[0].legend(['Training loss','Test loss'])\n",
    "\n",
    "ax[1].plot(train_acc_history,'g')\n",
    "ax[1].plot(test_acc_history,'b')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')  \n",
    "ax[1].legend(['Training Accuracy','Test accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
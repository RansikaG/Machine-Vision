{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,Y,b,W,reg):\n",
    "    #print(W.shape,\"W\")\n",
    "    #print(b.shape,\"b\")\n",
    "    #print(X.shape,\"X\")\n",
    "    #print(Y.shape,\"Y\")\n",
    "    y_hat=np.dot(X,W)+np.transpose(b)\n",
    "    #print(A.shape,\"A\")\n",
    "    m=X.shape[0]\n",
    "    #print(m)\n",
    "    mse =1/(2*m)* np.sum(np.square((Y - y_hat)**2))\n",
    "    Loss=mse+ 1/(2*m)*reg*np.sum(W**2)\n",
    "    cache={\"y_hat\":y_hat}\n",
    "    return Loss,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def back_prop(W,X,Y,Y_hat,reg):\n",
    "    m=Y.shape[0]\n",
    "    dy_hat =Y_hat-Y\n",
    "    #print(dy_hat.shape,\"dy_hat\")\n",
    "    #print(dA.shape,\"dA\")\n",
    "    dW =(1/float(m))*(np.dot(X.T, dy_hat)+reg*W)\n",
    "    db=(1/float(m))*np.sum(dy_hat,axis=0)\n",
    "    #print(dW.shape)\n",
    "    #print(db.shape)\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train: (50000, 32, 32, 3)\n",
      "x_test: (10000, 32, 32, 3)\n",
      "w1: (3072, 10)\n",
      "b1: (10,)\n",
      "Epoch 1/100>>> trainig_loss:  0.4999734143948544 testing_loss: 0.40553349908329633\n",
      "Epoch 2/100>>> trainig_loss:  0.4075498806643078 testing_loss: 0.48604410032033907\n",
      "Epoch 3/100>>> trainig_loss:  0.4825299064721808 testing_loss: 0.4589714005104501\n",
      "Epoch 4/100>>> trainig_loss:  0.4684749785246732 testing_loss: 1.322402442444237\n",
      "Epoch 5/100>>> trainig_loss:  1.288238008135077 testing_loss: 5.126919422102041\n",
      "Epoch 6/100>>> trainig_loss:  5.279145377605818 testing_loss: 42.90993343786812\n",
      "Epoch 7/100>>> trainig_loss:  42.21878912872143 testing_loss: 341.33377058390977\n",
      "Epoch 8/100>>> trainig_loss:  345.41199144687204 testing_loss: 2948.014237751858\n",
      "Epoch 9/100>>> trainig_loss:  2933.6886747481017 testing_loss: 23644.456585081094\n",
      "Epoch 10/100>>> trainig_loss:  23762.201317301395 testing_loss: 185370.22468981266\n",
      "Epoch 11/100>>> trainig_loss:  185210.74905026963 testing_loss: 1354834.5717372715\n",
      "Epoch 12/100>>> trainig_loss:  1358404.8183679963 testing_loss: 9369592.232972927\n",
      "Epoch 13/100>>> trainig_loss:  9374342.67776597 testing_loss: 60057309.877524234\n",
      "Epoch 14/100>>> trainig_loss:  60166804.09625031 testing_loss: 357232094.4651548\n",
      "Epoch 15/100>>> trainig_loss:  357586874.69598055 testing_loss: 1947639568.1909673\n",
      "Epoch 16/100>>> trainig_loss:  1950614023.8913295 testing_loss: 9693859998.857807\n",
      "Epoch 17/100>>> trainig_loss:  9705256942.71278 testing_loss: 43622724858.51112\n",
      "Epoch 18/100>>> trainig_loss:  43684350260.62071 testing_loss: 176368263050.41397\n",
      "Epoch 19/100>>> trainig_loss:  176588484515.4961 testing_loss: 634913101294.172\n",
      "Epoch 20/100>>> trainig_loss:  635780026009.603 testing_loss: 2019735213357.6045\n",
      "Epoch 21/100>>> trainig_loss:  2022319951771.698 testing_loss: 5625887856253.041\n",
      "Epoch 22/100>>> trainig_loss:  5633452827457.484 testing_loss: 13603658572220.41\n",
      "Epoch 23/100>>> trainig_loss:  13621258090792.012 testing_loss: 28278453801308.906\n",
      "Epoch 24/100>>> trainig_loss:  28316211107688.93 testing_loss: 50046769034891.93\n",
      "Epoch 25/100>>> trainig_loss:  50111834311095.89 testing_loss: 74603906831299.67\n",
      "Epoch 26/100>>> trainig_loss:  74703211474135.53 testing_loss: 92648108761392.4\n",
      "Epoch 27/100>>> trainig_loss:  92768779189070.67 testing_loss: 94699145386773.4\n",
      "Epoch 28/100>>> trainig_loss:  94825113917730.73 testing_loss: 78666481895785.27\n",
      "Epoch 29/100>>> trainig_loss:  78768899406885.03 testing_loss: 52369353013542.07\n",
      "Epoch 30/100>>> trainig_loss:  52439127842385.086 testing_loss: 27525219343823.664\n",
      "Epoch 31/100>>> trainig_loss:  27560936841355.934 testing_loss: 11232529008500.443\n",
      "Epoch 32/100>>> trainig_loss:  11247578436887.688 testing_loss: 3495224877705.7974\n",
      "Epoch 33/100>>> trainig_loss:  3499716376444.9116 testing_loss: 812297010222.2632\n",
      "Epoch 34/100>>> trainig_loss:  813402925998.2052 testing_loss: 137843564424.2002\n",
      "Epoch 35/100>>> trainig_loss:  138015343713.82812 testing_loss: 16625489539.264294\n",
      "Epoch 36/100>>> trainig_loss:  16649353352.502628 testing_loss: 1384124705.5587785\n",
      "Epoch 37/100>>> trainig_loss:  1385640371.303273 testing_loss: 76587355.59503098\n",
      "Epoch 38/100>>> trainig_loss:  76723102.02506581 testing_loss: 2711063.878133615\n",
      "Epoch 39/100>>> trainig_loss:  2711785.027832634 testing_loss: 57738.5296270033\n",
      "Epoch 40/100>>> trainig_loss:  57973.19330169896 testing_loss: 721.821525888084\n",
      "Epoch 41/100>>> trainig_loss:  716.8827037459173 testing_loss: 5.314301775671273\n",
      "Epoch 42/100>>> trainig_loss:  5.45971874560549 testing_loss: 0.35748801794530527\n",
      "Epoch 43/100>>> trainig_loss:  0.35219071073754016 testing_loss: 0.24728390924548385\n",
      "Epoch 44/100>>> trainig_loss:  0.24725543757991725 testing_loss: 0.2526287416996688\n",
      "Epoch 45/100>>> trainig_loss:  0.2519584229000893 testing_loss: 0.251102246834081\n",
      "Epoch 46/100>>> trainig_loss:  0.2505045487592228 testing_loss: 0.2507190061407494\n",
      "Epoch 47/100>>> trainig_loss:  0.25010554131008683 testing_loss: 0.25027653476530626\n",
      "Epoch 48/100>>> trainig_loss:  0.24965496891864625 testing_loss: 0.2498647888596557\n",
      "Epoch 49/100>>> trainig_loss:  0.2492352094129224 testing_loss: 0.2494785897205022\n",
      "Epoch 50/100>>> trainig_loss:  0.2488413638282993 testing_loss: 0.24911638007472017\n",
      "Epoch 51/100>>> trainig_loss:  0.2484718651570502 testing_loss: 0.24877670871378632\n",
      "Epoch 52/100>>> trainig_loss:  0.24812525190756946 testing_loss: 0.2484582185352962\n",
      "Epoch 53/100>>> trainig_loss:  0.2478001565343262 testing_loss: 0.24815963931220272\n",
      "Epoch 54/100>>> trainig_loss:  0.24749529825795435 testing_loss: 0.24787978120468818\n",
      "Epoch 55/100>>> trainig_loss:  0.24720947661425843 testing_loss: 0.24761752882817273\n",
      "Epoch 56/100>>> trainig_loss:  0.24694156555666252 testing_loss: 0.24737183582117156\n",
      "Epoch 57/100>>> trainig_loss:  0.24669050805651221 testing_loss: 0.2471417198689557\n",
      "Epoch 58/100>>> trainig_loss:  0.2464553111572203 testing_loss: 0.24692625814349237\n",
      "Epoch 59/100>>> trainig_loss:  0.24623504144273722 testing_loss: 0.2467245831236473\n",
      "Epoch 60/100>>> trainig_loss:  0.24602882088432754 testing_loss: 0.24653587876272004\n",
      "Epoch 61/100>>> trainig_loss:  0.2458358230327408 testing_loss: 0.24635937697318597\n",
      "Epoch 62/100>>> trainig_loss:  0.24565526952567254 testing_loss: 0.2461943544010579\n",
      "Epoch 63/100>>> trainig_loss:  0.24548642688295938 testing_loss: 0.2460401294645919\n",
      "Epoch 64/100>>> trainig_loss:  0.24532860356427302 testing_loss: 0.24589605963417654\n",
      "Epoch 65/100>>> trainig_loss:  0.24518114726619272 testing_loss: 0.24576153893217048\n",
      "Epoch 66/100>>> trainig_loss:  0.24504344243747356 testing_loss: 0.24563599563322416\n",
      "Epoch 67/100>>> trainig_loss:  0.24491490799309698 testing_loss: 0.24551889014724063\n",
      "Epoch 68/100>>> trainig_loss:  0.244794995209316 testing_loss: 0.24540971306862125\n",
      "Epoch 69/100>>> trainig_loss:  0.24468318578340204 testing_loss: 0.2453079833768147\n",
      "Epoch 70/100>>> trainig_loss:  0.24457899004317077 testing_loss: 0.24521324677445205\n",
      "Epoch 71/100>>> trainig_loss:  0.2444819452926411 testing_loss: 0.2451250741505168\n",
      "Epoch 72/100>>> trainig_loss:  0.24439161428133596 testing_loss: 0.24504306015708066\n",
      "Epoch 73/100>>> trainig_loss:  0.24430758378582906 testing_loss: 0.24496682188913074\n",
      "Epoch 74/100>>> trainig_loss:  0.24422946329313222 testing_loss: 0.2448959976579385\n",
      "Epoch 75/100>>> trainig_loss:  0.2441568837764413 testing_loss: 0.24483024584927898\n",
      "Epoch 76/100>>> trainig_loss:  0.2440894965546248 testing_loss: 0.2447692438585948\n",
      "Epoch 77/100>>> trainig_loss:  0.2440269722276171 testing_loss: 0.24471268709594024\n",
      "Epoch 78/100>>> trainig_loss:  0.24396899968062588 testing_loss: 0.2446602880542138\n",
      "Epoch 79/100>>> trainig_loss:  0.24391528515073024 testing_loss: 0.24461177543481805\n",
      "Epoch 80/100>>> trainig_loss:  0.24386555135008106 testing_loss: 0.24456689332546477\n",
      "Epoch 81/100>>> trainig_loss:  0.24381953664048897 testing_loss: 0.24452540042537768\n",
      "Epoch 82/100>>> trainig_loss:  0.24377699425472182 testing_loss: 0.24448706931363756\n",
      "Epoch 83/100>>> trainig_loss:  0.2437376915603204 testing_loss: 0.24445168575686482\n",
      "Epoch 84/100>>> trainig_loss:  0.243701409362196 testing_loss: 0.24441904805285178\n",
      "Epoch 85/100>>> trainig_loss:  0.24366794124068145 testing_loss: 0.24438896640713043\n",
      "Epoch 86/100>>> trainig_loss:  0.2436370929220843 testing_loss: 0.2443612623398089\n",
      "Epoch 87/100>>> trainig_loss:  0.2436086816791323 testing_loss: 0.24433576812032026\n",
      "Epoch 88/100>>> trainig_loss:  0.24358253575901226 testing_loss: 0.24431232622800822\n",
      "Epoch 89/100>>> trainig_loss:  0.24355849383697928 testing_loss: 0.24429078883672942\n",
      "Epoch 90/100>>> trainig_loss:  0.24353640449376684 testing_loss: 0.2442710173218756\n",
      "Epoch 91/100>>> trainig_loss:  0.24351612571524975 testing_loss: 0.24425288178842308\n",
      "Epoch 92/100>>> trainig_loss:  0.2434975244130096 testing_loss: 0.2442362606187925\n",
      "Epoch 93/100>>> trainig_loss:  0.24348047596463318 testing_loss: 0.24422104003945866\n",
      "Epoch 94/100>>> trainig_loss:  0.24346486377271756 testing_loss: 0.24420711370538895\n",
      "Epoch 95/100>>> trainig_loss:  0.24345057884169807 testing_loss: 0.2441943823015018\n",
      "Epoch 96/100>>> trainig_loss:  0.24343751937172767 testing_loss: 0.24418275316044155\n",
      "Epoch 97/100>>> trainig_loss:  0.24342559036892827 testing_loss: 0.24417213989605113\n",
      "Epoch 98/100>>> trainig_loss:  0.2434147032714291 testing_loss: 0.24416246205199224\n",
      "Epoch 99/100>>> trainig_loss:  0.24340477559066231 testing_loss: 0.24415364476502718\n",
      "Epoch 100/100>>> trainig_loss:  0.24339573056745545 testing_loss: 0.24414561844252067\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train:' , x_train.shape)\n",
    "print('x_test:' , x_test.shape)\n",
    "K = len(np.unique(y_train)) # Classes\n",
    "Ntr = x_train.shape[0]\n",
    "\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "#https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current\n",
    "x_test = x_test - mean_image\n",
    "\"\"\"In the ytrain it is preliminarily is in numbers just to indicate the particular class tf.keras.utils.to_categorical(y_train, num_classes=K) does is \n",
    "Converting the number to a binary vector\n",
    "Ex: \n",
    "a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "a = tf.constant(a, shape=[4, 4])\n",
    "print(a)\n",
    "tf.Tensor(\n",
    "[[1. 0. 0. 0.]\n",
    " [0. 1. 0. 0.]\n",
    " [0. 0. 1. 0.]\n",
    " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n",
    "\"\"\"\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "x_train = np.reshape(x_train,(Ntr,Din))#shaping images into a vector Din=32x32x3\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "std=1e-5\n",
    "w1 = std*np.random.randn(Din, K)#initialization\n",
    "b1 = np.zeros(K)\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "batch_size = Ntr #for the batch size whole dataset is considered\n",
    "\n",
    "iterations =100\n",
    "lr =0.05\n",
    "lr_decay=0.001#learning rate is decreased in each iteration\n",
    "reg =0#regularization term which stops growth of weights\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "for t in range(iterations):\n",
    "    #indices = np.arange(Ntr)#generating image\n",
    "    #rng.shuffle(indices)\n",
    "    # Forward pass\n",
    "    loss,cache=forward_prop(W=w1,b=b1,X=x_train,Y=y_train,reg=reg)\n",
    "    y_hat=cache[\"y_hat\"]\n",
    "    # Backward pass\n",
    "    dw1,db1=back_prop(Y_hat=y_hat,W=w1,X=x_train,Y=y_train,reg=reg)\n",
    "    w1=w1-lr*dw1\n",
    "    b1=b1-lr*db1\n",
    "    lr=lr/(1+t*lr_decay)\n",
    "    test_loss,_=forward_prop(W=w1,b=b1,X=x_test,Y=y_test,reg=reg)\n",
    "    print(\"Epoch \"+str(t+1)+\"/\"+str(iterations)+\">>> trainig_loss: \",loss,\"testing_loss:\",test_loss)\n",
    "# Printing accuracies and displaying w as images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n",
      "[1.5 0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "a = tf.constant(a, shape=[4, 4])\n",
    "print(a)\n",
    "B=[[1,2,1,1],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\n",
    "mse = np.mean((a - B)**2,axis=1)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "def loss(W,b,X,Y,reg):\n",
    "    print(W.shape,\"W\")\n",
    "    print(b.shape,\"b\")\n",
    "    print(X.shape,\"X\")\n",
    "    print(Y.shape,\"Y\")\n",
    "    prediction=np.dot(X,W)+np.transpose(b)\n",
    "    print()\n",
    "    m=X.shape[0]\n",
    "    mse = np.sum(np.square((Y - prediction)**2))\n",
    "    Loss=mse+ 1/(2*m)*reg*np.sum(W**2)\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.73105858, 0.88079708, 0.73105858, 0.73105858],\n",
       "       [0.5       , 0.73105858, 0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       , 0.73105858, 0.5       ],\n",
       "       [0.5       , 0.5       , 0.5       , 0.73105858]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    z=np.array(z)\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "sigmoid(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.49998923857004474\n"
     ]
    }
   ],
   "source": [
    "loss,cache=forward_prop(W=w1,b=b1,X=x_train,Y=y_train,reg=reg)\n",
    "print(loss)\n",
    "y_hat=cache[\"y_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[-5.60294253e-03, -2.38228143e-03,  2.25626789e-03, ...,\n",
       "         -1.66584013e-03, -6.09623615e-03, -1.43466064e-02],\n",
       "        [-9.87432428e-03, -8.76206678e-04,  1.98903826e-03, ...,\n",
       "         -2.10664917e-03, -1.04874780e-02, -1.61687181e-02],\n",
       "        [-1.79099455e-02, -1.37219492e-03,  6.71838721e-03, ...,\n",
       "         -1.92025827e-03, -1.81712554e-02, -2.04092694e-02],\n",
       "        ...,\n",
       "        [-1.25619369e-03, -2.77595903e-03,  5.82931016e-04, ...,\n",
       "         -5.32556245e-03,  1.18562262e-02, -5.05095503e-03],\n",
       "        [-3.97208294e-03, -2.21981253e-03, -4.58652880e-05, ...,\n",
       "         -3.55784550e-03,  6.31214200e-03, -3.90185596e-03],\n",
       "        [-9.14413372e-03, -4.84109105e-03,  2.94591984e-03, ...,\n",
       "          2.34340162e-03, -1.38188891e-03, -5.50754172e-03]]),\n",
       " array([-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1]))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "back_prop(Y_hat=y_hat,W=w1,X=x_train,Y=y_train,reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
